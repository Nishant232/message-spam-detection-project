{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbJaqaFbVCzD",
        "outputId": "55896251-17e1-4e22-e0fe-cde515e204c1"
      },
      "outputs": [],
      "source": [
        "#d!pip install pandas scikit-learn transformers openpyxl tensorflow\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import pipeline, TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer, AdamW, TFTrainer, TFTrainingArguments\n",
        "from transformers.convert_graph_to_onnx import convert\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eynYPq74VbNJ"
      },
      "outputs": [],
      "source": [
        "data = pd.read_excel('data.xlsx', engine='openpyxl')\n",
        "\n",
        "# Assuming the 'label' is in the first column and 'text' is in the second column.\n",
        "labels = data.iloc[:, 0]  # Get data from the first column\n",
        "texts = data.iloc[:, 1].str.lower()  # Lowercase all texts from the second column\n",
        "\n",
        "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "A9I7pXqIVeOa",
        "outputId": "b8fece64-7566-4610-df08-e792e7bd7532"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train = vectorizer.fit_transform(texts_train)\n",
        "X_test = vectorizer.transform(texts_test)\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, labels_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC9UIt5YVgAt",
        "outputId": "14f5b4fa-cb8f-4709-bd76-9856f2a24840"
      },
      "outputs": [],
      "source": [
        "preds = clf.predict(X_test)\n",
        "print(\"Naive Bayes Model Performance:\")\n",
        "print(classification_report(labels_test, preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgCW3udtViwl",
        "outputId": "a38277d8-7b04-49ae-e5ea-2fc7b28bf985"
      },
      "outputs": [],
      "source": [
        "# For simplicity, we are using the pre-trained model without fine-tuning\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, pipeline\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "hf_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "1sv6jpMmaUFi",
        "outputId": "a3c9b4a3-9a8b-4bad-acd7-59fb473f14a6"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train = vectorizer.fit_transform(texts_train)\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, labels_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slNECqnCVk0s",
        "outputId": "0e2583ba-9438-4f78-f53c-6f304e4a29ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Message: You've won a $1000 prize!\n",
            "Naive Bayes Prediction: Spam\n",
            "Huggingface Model Prediction: LABEL_1 (Confidence: 0.53)\n"
          ]
        }
      ],
      "source": [
        "message = \"You've won a $1000 prize!\"\n",
        "clf_pred = clf.predict(vectorizer.transform([message]))[0]\n",
        "hf_pred = hf_pipeline(message)\n",
        "\n",
        "print(f\"Message: {message}\")\n",
        "print(f\"Naive Bayes Prediction: {'Spam' if clf_pred else 'Not Spam'}\")\n",
        "print(f\"Huggingface Model Prediction: {hf_pred[0]['label']} (Confidence: {hf_pred[0]['score']:.2f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZW78kwTWCjw",
        "outputId": "a040b238-4e90-4eef-8c61-1ed5e29878d5"
      },
      "outputs": [],
      "source": [
        "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer, AdamW, TFTrainer, TFTrainingArguments\n",
        "from transformers.convert_graph_to_onnx import convert\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenize the dataset\n",
        "train_encodings = tokenizer(list(texts_train), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(list(texts_test), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Convert encodings to tf datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    list(labels_train)\n",
        ")).shuffle(1000).batch(32).repeat(2)  # Shuffle and batch the dataset\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    list(labels_test)\n",
        ")).batch(32)\n",
        "\n",
        "# Load model and set training arguments\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "# Fine-tune the model\n",
        "model.fit(train_dataset, epochs=2, validation_data=test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXH8Yu5oWDIR",
        "outputId": "090d29cc-2087-4e67-8605-60f8f6829d8b"
      },
      "outputs": [],
      "source": [
        "model.save(\"fine_tuned_model_directory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWqBNMy-WGPM",
        "outputId": "8e84d7db-ead3-42e7-d140-b4a08482f3e5"
      },
      "outputs": [],
      "source": [
        "loaded_model = tf.keras.models.load_model(\"fine_tuned_model_directory\")\n",
        "\n",
        "# Spam message to test the model\n",
        "message = \"hello,sir\"\n",
        "encoded_msg = tokenizer.encode_plus(\n",
        "    message,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128,\n",
        "    return_tensors=\"tf\"\n",
        ")\n",
        "\n",
        "input_data = {\n",
        "    \"input_ids\": encoded_msg[\"input_ids\"],\n",
        "    \"attention_mask\": encoded_msg[\"attention_mask\"],\n",
        "}\n",
        "\n",
        "output = loaded_model.predict(input_data)\n",
        "logits = output[\"logits\"] if \"logits\" in output else output[0]\n",
        "prediction = tf.argmax(logits, axis=1).numpy()[0]\n",
        "\n",
        "print(f\"Message: {message}\")\n",
        "print(f\"Loaded Model Prediction: {'Spam' if prediction else 'Not Spam'}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
